{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Additional Installation\n",
    "!pip install --user nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Initializing a spark context (ONLY ONCE)\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SQLContext\n",
    "conf = SparkConf().setAppName(\"building a warehouse\")\n",
    "sc = SparkContext(conf=conf)\n",
    "sqlCtx = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Functions to preprocess data. Also the training dataset is preprocessed.\n",
    "from nltk.corpus import stopwords\n",
    "import csv\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "#Retrieve the training data that we need to provide to the classifier\n",
    "def splitTrainingData(trainingSamples):\n",
    "    trainingSamples = csv.reader([trainingSamples], delimiter=',', quotechar='\"')\n",
    "    trainList = []\n",
    "    for line in trainingSamples:\n",
    "        trainList.append(line[0])\n",
    "        trainList.append(line[5])\n",
    "    return trainList\n",
    "data1 = sc.textFile(\"train.csv\").map(splitTrainingData)\n",
    "#Process the data retrieved\n",
    "def clean(trainingSamples):\n",
    "    tweet = trainingSamples[1]\n",
    "    tweet = re.sub('@[\\w]*','ATUSER',tweet)\n",
    "    tweet = tweet.lower()\n",
    "    tweet = re.sub('[0-9](\\w*)','',tweet)\n",
    "    tweet = re.sub(\"(http|https)://[\\w\\-]+(\\.[\\w\\-]+)+\\S*\", \" URL \", tweet)\n",
    "    tweet = re.sub(r'(.)\\1{2,}', r'\\1\\1', tweet)\n",
    "    trainingSamples[1] = tweet.translate ({ord(c): \" \" for c in \"!@#$%^&*()[]{};:,./<>?\\|~-=_+\"})\n",
    "    return trainingSamples\n",
    "def removeStopWords(trainingSamples):\n",
    "    tweet = trainingSamples[1]\n",
    "    stop = stopwords.words('english')\n",
    "    processedTweet = ''\n",
    "    for word in tweet.split(): # simple tokenization\n",
    "        if word not in stop:\n",
    "            if \"'\" not in word: \n",
    "                processedTweet = processedTweet + \" \" + word\n",
    "            else:\n",
    "                mylist = [i for i in word.split(\"'\") if i != '']\n",
    "                if(len(mylist)==1):\n",
    "                    processedTweet = processedTweet + \" \" + mylist[0]\n",
    "    trainingSamples[1] = processedTweet\n",
    "    return trainingSamples\n",
    "cleanData = data1.map(clean).map(removeStopWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Creating a LabeledPoint-(features, labels) to train the classifiers\n",
    "from pyspark.mllib.feature import HashingTF, IDF\n",
    "from pyspark.mllib.classification import NaiveBayes, NaiveBayesModel\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "\n",
    "labels = cleanData.map(lambda x: x[0], preservesPartitioning=True)\n",
    "tf = HashingTF().transform(cleanData.map(lambda x: x[1].split(), preservesPartitioning=True))\n",
    "training = labels.zip(tf).map(lambda x: LabeledPoint(x[0],x[1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Traning Naive bayes Model \n",
    "from pyspark.mllib.classification import NaiveBayes, NaiveBayesModel\n",
    "model = NaiveBayes.train(training, 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Testing the Naive Bayes Model performance(Training Data)\n",
    "predictionAndLabel = training.map(lambda p: (float(model.predict(p.features)), p.label))\n",
    "accuracy = 1.0 * predictionAndLabel.filter(lambda x: x[0] == x[1]).count() / training.count()\n",
    "\n",
    "from pyspark.mllib.classification import LogisticRegressionWithLBFGS\n",
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "\n",
    "metrics = MulticlassMetrics(predictionAndLabel)\n",
    "# Overall statistics\n",
    "resultMatrix = metrics.confusionMatrix().toArray()\n",
    "tp = resultMatrix[0][0]\n",
    "fp = resultMatrix[0][1]\n",
    "fn = resultMatrix[1][0]\n",
    "tn = resultMatrix[1][1]\n",
    "precision = tp/(tp+fp)\n",
    "recall = tp/(tp+fn)\n",
    "f1Score = (precision * recall)/(precision + recall)\n",
    "\n",
    "print(\"Naive Bayesian Classifier using training data:\")\n",
    "print (\"Accuracy = \" + str(accuracy * 100))\n",
    "print(\"Precision = %s\" % precision)\n",
    "print(\"Recall = %s\" % recall)\n",
    "print(\"F1 Score = %s\" % f1Score)\n",
    "\n",
    "print(\" \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Retrieving and processing test data - input to both the classifiers\n",
    "cleanTestData = sc.textFile(\"test.csv\").map(splitTrainingData).map(clean).map(removeStopWords)\n",
    "\n",
    "labelsTestData = cleanTestData.map(lambda x: x[0], preservesPartitioning=True)\n",
    "tfTestData = HashingTF().transform(cleanTestData.map(lambda x: x[1].split(), preservesPartitioning=True))\n",
    "\n",
    "testData = labelsTestData.zip(tfTestData).map(lambda x: LabeledPoint(x[0], x[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Testing the Naive Bayesian Model for performance(Test Data)\n",
    "predictTestData = testData.map(lambda p: (float(model.predict(p.features)), p.label))\n",
    "accuracyTestData = 1.0 * predictTestData.filter(lambda x: x[0] == x[1]).count() / testData.count()\n",
    "\n",
    "metricsTest = MulticlassMetrics(predictTestData )\n",
    "# Overall statistics\n",
    "resultMatrixTest = metricsTest.confusionMatrix().toArray()\n",
    "tpTest = resultMatrixTest[0][0]\n",
    "fpTest = resultMatrixTest[0][1]\n",
    "fnTest = resultMatrixTest[1][0]\n",
    "tnTest = resultMatrixTest[1][1]\n",
    "precisionTest = tpTest/(tpTest+fpTest)\n",
    "recallTest = tpTest/(tpTest+fnTest)\n",
    "f1ScoreTest = (precisionTest * recallTest)/(precisionTest + recallTest)\n",
    "print(\"Naive Bayesian Classifier using test data:\")\n",
    "print (\"Accuracy = \" + str(accuracyTestData * 100))\n",
    "print(\"Precision = %s\" % precisionTest)\n",
    "print(\"Recall = %s\" % recallTest)\n",
    "print(\"F1 Score = %s\" % f1ScoreTest)\n",
    "print(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Building the classifier two- Logistic Regression Model\n",
    "from pyspark.mllib.classification import LogisticRegressionWithLBFGS, LogisticRegressionModel\n",
    "modelTwo = LogisticRegressionWithLBFGS.train(training,iterations=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Testing performance of Logistic Classifier - using training data\n",
    "\n",
    "from pyspark.mllib.classification import LogisticRegressionWithLBFGS\n",
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "\n",
    "predictionAndLabelTwo = training.map(lambda p: (float(modelTwo.predict(p.features)), p.label))\n",
    "accuracyModelTwo = 1.0 * predictionAndLabelTwo.filter(lambda x: x[0] == x[1]).count() / training.count()\n",
    "\n",
    "metricsTwo = MulticlassMetrics(predictionAndLabelTwo)\n",
    "\n",
    "resultMatrixTwo = metricsTwo.confusionMatrix().toArray()\n",
    "tpTwo = resultMatrixTwo[0][0]\n",
    "fpTwo = resultMatrixTwo[0][1]\n",
    "fnTwo = resultMatrixTwo[1][0]\n",
    "tnTwo = resultMatrixTwo[1][1]\n",
    "precisionTwo = tpTwo/(tpTwo+fpTwo )\n",
    "recallTwo = tpTwo/(tpTwo+fnTwo)\n",
    "f1ScoreTwo = (precisionTwo * recallTwo)/(precisionTwo + recallTwo)\n",
    "print(\"Logistic Regression Classifier - using training data:\")\n",
    "print(\"Accuracy = \" + str(accuracyModelTwo * 100))\n",
    "print(\"Precision = %s\" % precisionTwo)\n",
    "print(\"Recall = %s\" % recallTwo)\n",
    "print(\"F1 Score = %s\" % f1ScoreTwo)\n",
    "\n",
    "print(\" \")\n",
    "\n",
    "#Testing performance of Logistic Classifier - using test data\n",
    "predictTestDataModelTwo = testData.map(lambda p: (float(modelTwo.predict(p.features)), p.label))\n",
    "accuracyTestDataModelTwo = 1.0 * predictTestDataModelTwo.filter(lambda x: x[0] == x[1]).count() / testData.count()\n",
    "\n",
    "metricsTwoTest = MulticlassMetrics(predictTestDataModelTwo)\n",
    "\n",
    "resultMatrixTwoTest = metricsTwoTest.confusionMatrix().toArray()\n",
    "tpTwoTest = resultMatrixTwoTest[0][0]\n",
    "fpTwoTest = resultMatrixTwoTest[0][1]\n",
    "fnTwoTest = resultMatrixTwoTest[1][0]\n",
    "tnTwoTest = resultMatrixTwoTest[1][1]\n",
    "precisionTwoTest = tpTwoTest/(tpTwoTest+fpTwoTest)\n",
    "recallTwoTest = tpTwoTest/(tpTwoTest+fnTwoTest)\n",
    "f1ScoreTwoTest = (precisionTwoTest * recallTwoTest)/(precisionTwoTest + recallTwoTest)\n",
    "print(\"Logistic Regression Classifier - using test data:\")\n",
    "print(\"Accuracy = \" + str(accuracyTestDataModelTwo  * 100))\n",
    "print(\"Precision = %s\" % precisionTwoTest)\n",
    "print(\"Recall = %s\" % recallTwoTest)\n",
    "print(\"F1 Score = %s\" % f1ScoreTwoTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
